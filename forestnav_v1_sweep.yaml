program: train_ppo.py
command:
  - ${interpreter}
  - ${program}
  - ${args_no_boolean_flags}
method: grid
name: ppo-parameter-sweep
metric:
  name: eval/episode_reward
  goal: maximize
parameters:
  # Parameters to sweep
  reward_scaling:
    values: [400.0, 800.0, 1200.0]
  learning_rate:
    values: [1e-4, 3e-4, 1e-3]
  entropy_cost:
    values: [1e-4, 1e-3, 1e-2]
  num_minibatches:
    values: [128, 256, 512]
  batch_size:
    values: [8, 16, 32]
  normalize_observations:
    values:
      - true
      - false

  # Fixed parameters
  num_timesteps:
    value: 2000000
  num_evals:
    value: 20
  episode_length:
    value: 1000
  unroll_length:
    value: 10
  num_updates_per_batch:
    value: 1
  num_envs:
    value: 128
  seed:
    value: 0
  action_repeat:
    value: 5
  discounting:
    value: 0.99
