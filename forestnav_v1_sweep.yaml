program: train_ppo.py
command:
  - ${interpreter}
  - ${program}
  - ${args_no_boolean_flags}
method: grid
name: ppo-parameter-sweep
metric:
  name: eval/episode_reward
  goal: maximize
parameters:
  # Parameters to sweep
  reward_scaling:
    values: [0.01, 0.1, 1, 10, 100]
  learning_rate:
    value: 0.001
  entropy_cost:
    value: 0.001
  num_minibatches:
    value: 256
  batch_size:
    value: 32
  normalize_observations:
    values:
      - false

  # Fixed parameters
  num_timesteps:
    value: 20000000
  num_evals:
    value: 20
  episode_length:
    value: 600
  unroll_length:
    values: [10, 20, 40, 80, 160]
  num_updates_per_batch:
    values: [1, 2, 4, 8]
  num_envs:
    value: 256
  seed:
    value: 0
  action_repeat:
    values: [1, 2, 4, 8, 16]
  discounting:
    value: 0.99
